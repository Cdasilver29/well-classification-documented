{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62057431",
   "metadata": {},
   "source": [
    "# Tanzania Water Wells â€“ Documented Notebook\n",
    "\n",
    "**Author:** Calvine Dasilver  \n",
    "**Project:** Machine Learning to Assess Water Well Performance in Tanzania\n",
    "\n",
    "This notebook includes a clean, documented version of the original analysis with:\n",
    "- Function-level docstrings\n",
    "- Markdown explanations\n",
    "- Step-by-step logic\n",
    "\n",
    "It is structured to improve readability and reusability for educational or open-source purposes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31afad",
   "metadata": {},
   "source": [
    "### Step 0: Import Required Libraries\n",
    "\n",
    "We import all necessary libraries for data manipulation, visualization, preprocessing, and model building. Scikit-learn is used for ML tasks and evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149bdd9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, mean_squared_error\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8479dc",
   "metadata": {},
   "source": [
    "### Step 1: Define Data Loading Function\n",
    "\n",
    "We define a reusable function to load a CSV dataset, inspect its shape, and display basic structure info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f3764",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_and_check_info(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file and get information about the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - df_info (str): Information about the DataFrame.\n",
    "    \"\"\"\n",
    "    df_1 = pd.read_csv(file_path, index_col='id')\n",
    "    df_head = df_1.head()\n",
    "    df_info = df_1.info()\n",
    "    df_shape = df_1.shape\n",
    "    print(\"Data Shape:\", df_shape)\n",
    "    return df_1, df_info, df_head, df_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28f4430",
   "metadata": {},
   "source": [
    "### Step 2: Define Function to Check Data Types & Missing Values\n",
    "\n",
    "This function summarizes the data types and missing values for all features, helping us plan data cleaning strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e705e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def check_data_types_and_missing_values(data):\n",
    "    \"\"\"\n",
    "    Analyze data types and missing values for a given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataset to inspect.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing data type summary and missing value count.\n",
    "    \"\"\"\n",
    "    data_types = data.dtypes.replace({'object': 'string'}).value_counts().to_dict()\n",
    "    missing_values = data.isnull().sum()\n",
    "    return {\"data_types\": data_types, \"missing_values\": missing_values}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768e0b33",
   "metadata": {},
   "source": [
    "### Step 3: Load Training Data\n",
    "\n",
    "We use the earlier function to load the training feature dataset and preview its structure and shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930b317",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "file_path_1 = \"Data/Training_set_values.csv\"\n",
    "df_1, df_info, df_head, df_shape = load_data_and_check_info(file_path_1)\n",
    "print(\"Shape of the DataFrame:\", df_shape)\n",
    "print(df_info)\n",
    "df_head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacf6e6",
   "metadata": {},
   "source": [
    "### Step 4: Explore Missing Data and Column Types\n",
    "\n",
    "We apply our function to evaluate how many columns have missing data and what types they are. This informs our cleaning strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe76f83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "dtypes_and_mv_df_1 = check_data_types_and_missing_values(df_1)\n",
    "print(\"Data types of each column:\")\n",
    "print(dtypes_and_mv_df_1[\"data_types\"])\n",
    "\n",
    "if dtypes_and_mv_df_1[\"missing_values\"].sum() == 0:\n",
    "    print(\"\\nNo missing values found.\")\n",
    "else:\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(dtypes_and_mv_df_1[\"missing_values\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036ccb0",
   "metadata": {},
   "source": [
    "### Step 5: Load and Merge Labels with Feature Set\n",
    "\n",
    "We now load the label dataset and merge it with the feature dataset (`df_1`) using the index (`id`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a40d8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_2 = pd.read_csv(\"Data/Training_set_labels.csv\", index_col='id')\n",
    "merged_df = df_1.merge(df_2, how='inner', left_index=True, right_index=True)\n",
    "print(\"Shape after merge:\", merged_df.shape)\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae8e90",
   "metadata": {},
   "source": [
    "### Step 6: Target Variable Distribution\n",
    "\n",
    "Understanding class balance helps guide our modeling strategy. Here we inspect the distribution of the target variable `status_group`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b6a04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=merged_df, x='status_group')\n",
    "plt.title('Target Class Distribution')\n",
    "plt.xlabel('Status Group')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f5adb",
   "metadata": {},
   "source": [
    "### Step 7: Drop Redundant or Sparse Features\n",
    "\n",
    "Based on domain knowledge or null values, we drop columns that are unlikely to contribute meaningfully to predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da394181",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['scheme_name', 'recorded_by', 'region_code', 'wpt_name']\n",
    "merged_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c5a66",
   "metadata": {},
   "source": [
    "### Step 8: Fill Missing Values\n",
    "\n",
    "We use median or forward-fill methods to handle missing data. This is crucial for avoiding errors during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b2ed8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "merged_df['construction_year'].replace(0, np.nan, inplace=True)\n",
    "merged_df['construction_year'].fillna(merged_df['construction_year'].median(), inplace=True)\n",
    "\n",
    "merged_df['public_meeting'].fillna(method='ffill', inplace=True)\n",
    "merged_df['permit'].fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7a02a",
   "metadata": {},
   "source": [
    "### Step 9: Encode Categorical Variables\n",
    "\n",
    "We convert string-based categorical columns into numeric format using Label Encoding, allowing compatibility with ML models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c316a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in merged_df.select_dtypes(include='object').columns:\n",
    "    merged_df[col] = le.fit_transform(merged_df[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9815e",
   "metadata": {},
   "source": [
    "### Step 10: Split Features and Target\n",
    "\n",
    "We prepare the independent variables `X` and dependent variable `y` before training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d13dfdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X = merged_df.drop('status_group', axis=1)\n",
    "y = merged_df['status_group']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f9913",
   "metadata": {},
   "source": [
    "### Step 11: Split Dataset into Training and Testing Sets\n",
    "\n",
    "We split the dataset into training and test subsets using an 80/20 ratio. This allows us to evaluate model performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703bcf3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a682222",
   "metadata": {},
   "source": [
    "### Step 12: Train Baseline Models\n",
    "\n",
    "We train multiple machine learning classifiers to compare performance:\n",
    "- Logistic Regression\n",
    "- Decision Tree\n",
    "- Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9878ee6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"\\n{name} Results\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b07f9",
   "metadata": {},
   "source": [
    "### Step 13: Visualize Confusion Matrix for Best Model\n",
    "\n",
    "A confusion matrix helps visually inspect model prediction performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560b6cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "best_model = models[\"Random Forest\"]\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "disp = ConfusionMatrixDisplay.from_estimator(\n",
    "    best_model, X_test, y_test, cmap='Blues', xticks_rotation='vertical'\n",
    ")\n",
    "disp.ax_.set_title(\"Confusion Matrix - Random Forest\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf0f09a",
   "metadata": {},
   "source": [
    "### Step 14: Feature Importance from Random Forest\n",
    "\n",
    "We identify which features contributed most to predictions using `.feature_importances_`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187bb77b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "importances = best_model.feature_importances_\n",
    "features = X.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "sns.barplot(x=importances[indices][:10], y=features[indices][:10])\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
